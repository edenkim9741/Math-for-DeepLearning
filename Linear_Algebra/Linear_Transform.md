# 선형 변환과 행렬
지난 시간에 말했듯 선형 변환은 행렬을 통해 표현할 수 있음. 여러 행렬에 대해서 선형 변환 되었을 때 어떤 모습으로 변환될 지 생각해보자

$$ \mathbf{A}_1 = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix} $$

$$ \mathbf{A}_2 = \begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix} $$

$$ \mathbf{A}_3 = \begin{bmatrix}
0 & 2 \\
1 & 0
\end{bmatrix} $$

$$ \mathbf{A}_4 = \begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix} $$

$$ \mathbf{A}_5 = \begin{bmatrix}
1 & 0 \\
1 & 0 
\end{bmatrix} $$

# 선형대수로 바라본 Layer
딥러닝에서 Layer는 선형 변환을 통해 표현할 수 있음
perceptron은 행렬 곱으로 표현될 수 있나? => perceptron을 그려보고 이를 행렬로 표현해보자

$$ \mathbf{A} = \begin{bmatrix}
a & b \\
c & d
\end{bmatrix} $$

만약 activation function이 있다고 하면 어떨까?


# 선형 변환과 위상 동형 사상
선형 변환은 위상 동형 사상의 특수한 경우로 볼 수 있음

위상 동형 사상이란, 위상 공간을 일대일 대응하여, 연속적이고 역도 연속인 함수를 통해 변환하는 것이라고 생각할 수 있는데



여기서 한 가지 재미있는 사실은, 우리가 사용하는 Activation Function이 이 일대일 대응이고 연속이며 역도 연속인 조건을 모두 만족하는 함수만을 사용한다는 점

우리는 이것을 비선형 변환이라고 부르지만, 사실은 위상 동형 사상이라고 볼 수 있음

우리는 ReLU, Sigmoid, Tanh 등과 같은 Activation Function을 직관에 의해 선택하고 사용했는데, 이는 모두 매끄러운 매니폴드 가정을 지키는 한에서의 변환이었던 것.


# 분포를 이용한 선형 변환
지난 시간에 알아봤듯 sample의 분포는 공분산 행렬을 통해 표현할 수 있음
고유벡터가 분포의 방향 (즉 타원의 축)을 의미하고 고유값이 해당 축의 길이를 의미하는 것을 살펴 봤음

고유벡터가 변환 후에도 변하지 않는 벡터를 의미한다는 것을 생각해보면 공분산 행렬이 분산을 반영한다는 것을 다시 한 번 이해할 수 있음

그럼 만약 아래와 같이 타원으로 몰려있는 분표가 있다고 가정해보자.
![](attachments/Pasted%20image%2020250424180114.png)

초기 분포를 원이라고 가정하면 이 분포가 어떻게 만들여졌는지 생각해볼 수 있음
=> 해당 분포를 만드는 공분산 행렬을 선형 변환 행렬로 사용하면 타원 모양을 만들 수 있음

# Gauss-seidel, Jacobi 반복법
Gauss-seidel, Jacobi 반복법은 선형 방정식의 해를 구하는 방법으로, 행렬을 이용한 선형 변환을 통해 표현할 수 있음
이 방법들은 선형 방정식의 해를 구하는 데 사용되는 반복적인 알고리즘으로, 행렬을 이용한 선형 변환을 통해 표현할 수 있음